[TOC]

## 1 概念与思维

推荐系统就是用已有的连接去预测未来用户和物品之间将会出现的连接。建立连接时，用户之间、物品之间、用户和物品之间，都形成网络，直到网络形成长尾作用，推荐系统才能发挥作用

### 预测问题模式

#### 评分预测

预测连接的强度（预测用户消费体验）。常见于点评类产品（书影音），属于显式反馈

缺点

* 数据收集不易。用户需要从看到一直到消费完成并给出评分
* 数据质量不能保证。伪造数据门槛低，真实数据收集不易，门槛高（？？？不通）
* 数据不稳定，评分在不同时期差别大

#### 行为预测

预测是否会产生连接（预测用户是否会完成消费），隐式反馈，利用隐式反馈数据预测隐式反馈发生的概率。从登陆到购买收藏，都是用户行为，数据量比显式反馈多很多，

常见两种预测方式：

1. CTR（点击率）预估：直接预测行为本身发生的概率。点击率可引申到收藏，购买等行为
2. 预测物品的相对排序

隐式反馈优点：1)数据稠密。2)蕴藏了更多用户兴趣。3）常常和产品指标挂钩，容易做AB实验，容易与模型的目标函数关联

给出高评分的先决条件是用户要有评分行为，所以行为预测解决推荐系统80%的问题，评分预测解决最后20%的问题。（？？？）

### 顽疾

没有通用解决方案，不容易被重视的问题

* 冷启动：新用户或不活跃用户、新物品或展示次数少的物品，缺乏相关数据
* 探索与利用（EE）问题：假设已经知道用户喜好，如何平衡用户感兴趣的内容和探索用户新的兴趣
* 安全问题：系统被攻击可能造成的影响：给出不可靠的推荐结果、收集了不可靠的脏数据，难以完全消除

### 思维模式

四个关键元素需要注意，重要性依次递减

* UI和UE：颜值最重要
* 数据
* 领域知识：其他产品无法替代的部分（电商——价格，新闻——快）
* 算法：产品初期算法不重要，更重要的是产品和运营

目标思维：量化一切。量化目标，不能停留在“感觉很准”的层面，要设定一个数据目标，不断迭代来达到。要量化就要收集数据，得到结果。要有目标和约束，例如目标是点击率，多样性为约束，不能为了点击率不顾多样性，要设定一个阈值，让每次推荐排序在计算最佳排序的结果时，多样性的量化值不能低于阈值，在满足约束的情况下，目标值越大（小）越好，在产品的不同阶段，目标和约束可以互相切换

不确定性思维：用概率的眼光看待结果。推荐系统绝大多数算法都是概率算法，无法保证得到准确的结果，只能提高得到好结果的概率；如果为了一个很难出现的事件花时间修补，得不偿失；偶尔出现不可解释的推荐也是有益的，可以探索用户新兴趣。也不应该把一切bug归咎于不确定

## 2 产品漫谈

推荐系统是吸引注意力的重要手段

存储注意力：基于用户兴趣筛选信息流，重新排序展示，吸引注意力，防止注意力衰减（类似微博的关注，先把感兴趣的排在前面，而不是按时间显示）

## 3 内容推荐

### 用户画像

推荐系统的用户画像是给机器看的。推荐系统就是要在用户和物品之间建立连接，一般，对用户和物品之间的匹配来评分，预测用户评分或偏好，评分前，需要将物品和用户都表示成向量，才能计算，推荐算法不同，向量化的方式也不同，匹配评分的做法也不同

用户向量化的结果就是用户画像，所以向量化是必须做的。除了常见的用户资料、标签，还有各种深度学习得到的Embbedding向量，都是很好的画像内容

大型推荐系统的实现一般分为召回和排序两个阶段。全量物品数量庞大，无法为每个用户逐一计算每个物品的评分，这时需要召回阶段，预先筛选一部分物品，降低计算量，用户画像除了用于最终匹配，还用于召回。所以构建用户画像要以这两个阶段为目的

向量有两种

* 稀疏向量：标签、注册资料、社交关系。是显而易见的用户画像，可解释性更好，可以用来给出推荐理由。
* 稠密向量：通过训练神经网络得到的Embedding向量、通过矩阵分解得到的隐因子、浅层语义分析或者话题模型得到的话题分布等。能抓住更多隐藏兴趣，但黑盒子特点也导致了它的可解释性不如前者

建立用户画像的关键因素，向量的关键因素

* 维度（价格、种类、味道等维度）
  * 每个维度的名称都是可理解的。用户画像的价格维度和商品的价格维度是可以相对应相匹配的。将用户画像和物品同纬度相乘，可理解为用户画像这边的值是权重，给物品画像那边的值加权
  * 维度的数量：根据用户阅读历史挖掘兴趣标签，那么标签的种类数就是维度数，如果维度是一些Embedding向量，则维度数就是根据模型的结构来定的
  * 维度：维度数不是必须的，用户画像中稀疏向量的维度数可以不用提前确定。维度越多画像越精细，但是计算代价也越大，需要权衡。用户画像是向量化结果，不是标签化

* 量化：用户每个维度的量化都应该交给机器，主观量化是大忌，不要为了用户画像而用户画像，它只是推荐系统的一个副产品，应根据效果（排序好坏、召回覆盖等指标）指导用户画像的量化

用户画像的构建方法有以下3类

1. 查户口。个人信息、购买历史等作为画像内容，实现简单，但对于冷启动非常有用
2. 堆数据。堆积历史数据作统计工作，是最常见的用户画像获取方式，常见的兴趣标签就是这一类。先给物品打上标签，在获取用户在这些标签上的历史行为，从标签维度做数据统计，用统计结果作为量化结果
3. 黑盒子。用机器学习的方法，学习稠密向量。作用大，比如使用潜语义模型构建用户阅读兴趣，以及使用矩阵分解得到的隐因子或深度学习模型学习用户的Embedding向量。这一类用户画像数据因为不可解释，不能直接被人看懂

### 标签挖掘技术

早期的推荐系统一般都从基于内容的推荐系统做起，需要从内容本身挖掘用户兴趣标签

主要处理文本数据，用户端：

* 个人信息
* 产生的内容
* 与用户发生了连接关系的文本，如阅读记录

物品端：

* 标题、描述
* 物品本身的内容
* 物品基本属性中的文本

基础版本用户画像需要做的事：

1. 把所有非结构化的文本结构化、去粗取精，保留关键信息，构建高质量标签库

   覆盖面：标签库能覆盖的用户、物品越多越好

   健康度：量化标签平均覆盖物品的程度

   经济性：标签之间的语义相似性越小越好。两个有相同语义的标签却占据了两个兴趣向量的维度，性价比低

2. 根据用户资料、行为、内容，把标签传递给用户

   标签库是用户和物品共享的，因为标签库是为了给用户找到感兴趣的物品，所以不能用户和物品两遍各做一套

3. 定期更新

构建标签库分为两派

* 中心化：专业分类法（人工分类），这样的标签库表现为一个分类树，需要较多人员维护
* 去中心化：1）大众分类法，依靠用户贡献标签，比如豆瓣书影音标签。2）源于挖掘的标签：从非结构化的文本中抽取标签后构建标签库

| 对比维度       | 中心化         | 去中心化           |
| -------------- | -------------- | ------------------ |
| 标签覆盖面     | 小             | 大                 |
| 标签覆盖健康度 | 好（均匀）     | 不好（倾斜）       |
| 标签经济性     | 好（相对独立） | 不好（同义近义多） |

高质量的标签库需要两者结合使用。注意几点

* 不同角度构建专业分类体系（例如图片除了静态图片、动态图片等形式分类，还要有娱乐、体育等主题分类）
* 去中心化的标签要归一化，同义标签保留最常用的
* 去中心化的标签也需要专业人员把控质量

### 标签挖掘方法

结构化文本。利用NLP自然语言处理算法分析物品段的文本信息可得到：

* 关键词提取：最基础的标签来源，也为其他文本分析提供基础数据，常用TF-IDF和TextRank
* 实体识别：人物、位置、著作、影视剧、历史事件、热点事件等，常用基于词典的方法结合CRF（条件随机场）模型
* 内容分类：按照分类体系分类，用分类表达粗粒度的结构化信息
* 文本聚类：在无人制定分类体系的前提下，无监督地将文本划分成多个类簇。不是标签，但也是用户画像常用构成
* 主题模型：从大量已有文本中学习主题向量，再预测新的文本在各个主题上的分布概率。也是一种聚类思想。不是标签，但也是用户画像常用构成
* 嵌入：Embeddinng，从词到篇章，都可以学习其嵌入表达，可挖掘出字面意思之下的语义信息，并用有限的维度表达出来

#### 关键词提取

两种方法都是无需标注数据，属于无监督的方式。获取的标签都是稀疏的，为了覆盖面更广，需要引入大量近义标签，Embedding可解决这些问题

##### TF-IDF

在一篇文章中反复出现的词重要，在所有文档中都出现的词不重要

TF词频：某个词在文本中出现的次数，文本越大越有用

IDF逆文本频率指数：提前统计好的，在已有文档中统计某个词出现在多少文档中

两值相乘获取权重，根据权重筛选关键词的几种方式：

* 给定一个K，取topK个词，但如果词的个数小于k，所有都是关键词，不合理
* 计算平均值，取权重在均值之上的值
* 设定阈值，保留阈值之上的词
* 其他过滤措施：只提取动词和名词等

##### TextRank

词和词的关系为无向图，每个词把自己的权重平均分给和自己有连接的其他词，每个词把其他词分配给自己的权重求和，作为新权重

#### Embedding向量 Word2Vec

也叫嵌入，能通过学习得到每个词低纬度的稠密向量，就可以计算词之间的距离，实现标签归一化，提高标签库的经济性。还可用于 文本分类和聚类，得到更抽象的标签。根据用户行为直接学习到的向量本身就可以作为用户画像的一部分

一个词可能有包含很多语义信息，北京包含首都、中国、直辖市等语义，这些语义是有限的，比如128个，那么这个词就用一个128维的向量表达，各个维度的大小表示词包含这个语义的量

Word2Vec可通过浅层神经网络学习得到每个词的向量表达，百万词汇几分钟内跑完。工具有python的gensim，可对语料库批量训练，还能以数据流的方式在线更新模型

```python
from gensim.models import Word2Vec
import sys
import argparse

class Embedding(object):
    def __init__(self, size=128, window=5, min_count=5, workers=2, epochs=10, pretrained_model=None):
        """
        训练词的Embedding向量
        :param size: 向量维度
        :param window: 窗口长度
        :param min_count: 最小词频
        :param workers: 并行化
        :param epochs: 迭代次数
        :param pretrained_model: 预训练的模型
        """
        self._size = size
        self._window = window
        self._min_count = min_count
        self._workers = workers
        self._epochs = epochs
        if pretrained_model:
            self._model = Word2Vec.load(pretrained_model)

    def train(self, sentences=[]):
        if self._model:
            self._model.train(sentences,
                              total_examples=len(sentences),
                              epochs=self._epochs)
        else:
            self._model = Word2Vec(sentences,
                                   self._size,
                                   window=self._window,
                                   min_count=self._min_count,
                                   workers=self._workers)

    @property
    def model(self):
        return self._model
```

得到词的Embedding向量后，就可对原本标签库进行扩展或者归一化：相同语义的词只保留标准的，提高经济型

#### 文本分类 FastText

咨询内容需要将内容自动分类到不同频道中，得到最粗粒度的结构化信息，可用来在冷启动时探索用户兴趣

长文本分类容易，短文本困难。短文本的经典算法是SVM，工具是Facebook的FastText——学习词语的Embedding向量和文本分类，尤其是句子分类。可给句子打标签，输出多个标签

可以不分词，但是分词之后效果更好，样本格式要求：一行一个样本，类名名字随便写，但是需要__label__前缀，用空格和正文分开

```python
import fasttext
tag_extractor = fasttext.supervised('train.txt', 'model.bin', label_prefix="__label__")
result = tag_extractor.test('test.txt')
print(result.precision)
print(result.recall)
```

#### 命名实体识别

NER，识别有特定意义的实体，地名、电影名书名等。非常有价值，有利于构建高质量标签库

经典的模型有HMM、CRF，深度学习的BiLSTM结合CRF有非常好的效果。还有非模型做法：词典法提前准备好各种实体的地点，使用tire-tree存储，拿分好的词去词典找。CRF++是CRF的开源

```
# 训练模型
crf_learn 模板 模板语料 模板文件
# 从文本识别
crf_learn -m 模板文件 测试文件
```

#### 文本聚类

将文本聚成几堆，每堆都有相似语义。类之间更独立，有经济性

1. 对全量文本聚类，得到每个类的中心
2. 将新的文本加入距离最近的类中心所在的类
3. 每个类赋予一个id，从该类中找出最能代表该类的主题词作为类别标签

方法：基于距离的聚类方法（Kmeans）计算复杂度大，效果不理想。多选择主题模型或者隐语义模型（LSI），LDA模型能更准确的抓住主题，能得到软聚类的方式（一条文本属于多个类）

LDA需要设定主题个数k，k可根据多次实验对比来选择：每次计算k个主题两两相似度的平均值，选择一个较低k。如果计算资源够用，k可选择较大的值。

开源的LDA训练工具有LightLDA、gemsim、PLDA，以gemsim为例

```python
from gensim.models.ldamodel import LdaModel
from gensim.corpora.dictionary import Dictionary


class LDA(object):
    def __init__(self, topics=10,
                 worker=3,
                 pretrained_model=None,
                 dictionary=None):
        """
        lda训练模型初始化
        :param topics: 主题个数
        :param worker: 并行化参数，一般为core数量-1
        :param pretrained_model: 预训练的模型，由于支持在线更新，可加载上次训练的模型
        :param dictionary: 训练时词需要转换成ID，所以跟模型配套有一个ID映射的词典
        """
        self._topics = topics
        self._worker = worker
        self._model = None
        self._common_dictionary = None
        if pretrained_model and dictionary:
            self._model = LdaModel.load(pretrained_model)
            self._common_dictionary = Dictionary.load(dictionary)

    def save(self, model_file, dictionary_file):
        """
        保存训练的模型，同时保存对应词典
        :param model_file: 模型文件
        :param dictionary_file: 词典文件
        :return:
        """
        if self._model:
            self._model.save(model_file)
        if self._common_dictionary:
            self._common_dictionary.save(dictionary_file)

    def update(self, corpus=[]):
        """
        在已有模型的基础上在线更新
        :param corpus: 用于更新的文档列表
        :return: 
        """
        if not self._model and len(corpus) > 0:
            print('train from scratch...')
            self._common_dictionary = Dictionary(corpus)
            corpus_data = [self._common_dictionary.doc2bow(sentence) for sentence in corpus]
            self._model = LdaModel(corpus_data, self._topics)
        elif self._model and len(corpus) > 0:
            self._common_dictionary.add_documents(corpus)
            new_corpus_data = [self._common_dictionary.doc2bow(sentence) for sentence in corpus]
            self._model.update(new_corpus_data)

    def inference(self, document=[]):
        """
        推断新文档的话题发布
        :param document: 文档（词列表） 
        :return: 话题分布列表 
        """
        if self._model:
            doc = [self._common_dictionary.doc2bow(document)]
            return self._model.get_document_topics(doc)
        return []

    @property
    def model(self):
        return self._model

    @property
    def dictionary(self):
        return self._common_dictionary


# 使用
lda = LDA(topics=20, worker=2, pretrained_model=model_file, dictionary=dic_file)
corpus = read_file(corpus_file)
lda.update(corpus)
lda.save(model_file, dic_file)
topics = lda.inference(['word1', 'word2'])
```

#### 标签选择

把物品的结构化信息传递给用户（？？？哪里传递了）

从用户对物品的行为中，挑选用户感兴趣的物品特征

下面两种方法都是有监督的特征选择方法，需要提供分类标注信息。思想：

* 把用户的结构化内容看成文档
* 把用户对物品的行为看成类别
* 每个用户见过的物品就是一个文本集合
* 在这个文本集合上使用特征选择算法选出每个用户关心的东西

**卡方检验**

本质是检测某个词和某个类别相互独立，如果和假设偏离越大，越说明有关联，就是关键词

**信息增益** IG

信息熵

* 各个类别的文本数量差不多时，信息熵较大
* 其中少数类别的文本数量明显较多时，信息熵小

IG计算步骤

1. 统计全局文本信息熵，通常是按照类别的分布计算
2. 统计每个词的条件熵
3. 两者相减就是词的信息增益

在数据挖掘中的决策树分类算法中应用得最多。卡方检验针对每个分类单独筛选一套标签出来，后者是全局统一筛选。

都是在离线层批量完成的，每天更新一次，次日就可以使用新的用户画像。新用户快速生成画像需要用到MAB（多臂老虎机）

### 基于内容的推荐

就是包装成推荐系统的信息检索系统，处在推荐系统的初级

##### 抓

抓数据，补充内容源，增加分析维度

##### 洗

冗余、垃圾、政治、色情

##### 挖（内容分析）

对已有内容的深入挖掘是基于内容的推荐系统最重要的一步，只要内容挖掘足够深，即使推荐算法一般，也能取得好效果。

1. 通过文本进行分类（如娱乐类）
2. 分析文本的主题
3. 识别内容的主角（如吴亦凡）
4. Embedding向量分析，抓住潜藏的语义，更能精确表达内容

内容分析的产出：

* 结构化的内容库：结合用户行为去学习用户画像（之前的算法）

* 内容分析模型：模型如：

  * 分类模型
  * 主题模型
  * 实体识别模型
  * Embedding模型

  它们的主要应用场景：新的物品刚刚进入时，需要实时地被推荐出去，要对内容进行实时分析、提取结构化内容，再去和用户画像做比对

##### 算

将用户的兴趣和物品的属性相匹配

最简单的方法是计算相似性：用户的画像内容表示为稀疏向量，内容端也有对应的稀疏向量，两者之间计算余弦相似度，根据相似度对推荐物品排序

借鉴信息检索中的相关性计算方法来推荐匹配计算，BM25F算法

两种算法都可快速实现，但不属于学习型算法，因为目标没有最优化

学习型算法的思路：典型的场景是提高行为的转化率，如点击、转发，收集这类行为的日志，转换成训练样本，训练预估模型。样本由两部分构成：特征，包含用户画像、物品的结构化内容，还可加入场景信息如时间、位置、设备。另一部分是用户行为。作为标注信息，包含有反馈和无反馈两类。用这样的样本训练一个二分器，常用模型是逻辑回归LR和梯度提升树GBDT，或两者结合。在推荐匹配时，预估用户行为发生的概率，并按照概率排序。可一直迭代优化下去

## 4 近邻推荐

### 协同过滤

重点在协同，协同过滤算法简单直接、历久弥新

基于内容的推荐系统一段时间后，就有可观的用户行为了。用户行为可表达成一个用户和物品的关系矩阵，或叫网络或图。填充的就是用户对物品的态度，单不一定每个位置都有内容，需要把没有内容的地方填满。这个关系矩阵是协同过滤的命根子，一切围绕它来进行

基于记忆的协同过滤 Memory-Based：记住每个用户消费过什么，然后给他推荐相似的东西，或把相似的人喜欢的东西推荐给他

基于模型的协同过滤 Model-Based：从用户和物品的关系矩阵中学习一个可以泛化的模型，从而把那些矩阵空白处填满

### 基于用户的协同过滤

是基于记忆的协同过滤算法之一。根据历史消费行为找到一群和你口味相近的用户，把他们消费的新的、你妹见过的物品推荐给你。给用户聚类，把用户按照兴趣口味聚类成不同的群体，给用户的推荐就来自这个群体。做好的关键是如何量化"口味相似"

核心是用户和物品的关系矩阵，一切围绕这个矩阵进行

1. 使用行为数据构建用户向量，有行为数据的用户才会有，每个用户的向量的特点：
   * 向量的维度是物品的个数
   * 向量是稀疏的，不是每个维度都有数值（用户可能还没见过这个物品）
   * 向量维度的取值可以是简单的0或1的布尔值，1表示见过且喜欢，0表示见过且不喜欢，无表示没见过，因为是稀疏向量，取值为0的就忽略了
2. 根据用户向量，两两计算用户的相似度。设定一个阈值或最大数量，为每个用户保留最相似的用户。把原始的行为向量转换为相似用户向量
3. 为每个用户产生推荐结果。把他相似用户喜欢过的物品汇总起来，去掉已消费过的物品，剩下的排序输出就是推荐结果

场景：

1. 相似用户列表：可以推荐口味相同的人
2. 基于用户的推荐结果

缺点：

1. 用户数量多时，两两相似度计算起来非常吃力，成为瓶颈
2. 用户品味变化快，很难被反映出来
3. 由于数据稀疏和倾斜，用户和用户共同消费行为比较少，而且一般都是热门物品，对发现用户兴趣不大

#### 将行为日志构造矩阵（矩阵表示）

行为日志矩阵

|       | item1 | item2 | item3 |
| :---: | :---: | :---: | :---: |
| user1 |   ?   |   1   |   0   |
| user2 |   0   |   ?   |   1   |
| user3 |   1   |   0   |   0   |
| user4 |   ?   |   ?   |   1   |

因为矩阵是稀疏的，很多元素不用存，使用下面两种稀疏矩阵存储格式（压缩），Spark和Numpy都支持

* CSR：稍复杂，采用整体编码方式，由数值、列号、行偏移共同编码组成
* COO：存储方式简单，每个元素用一个三元组表示（行号、列号、数值），只存储有值的元素

#### 相似用户计算

首先计算单个相似度（向量两两相似度），为了应对长向量带来过多的遍历，通常计算相似度、复杂度是通过采样算法加速相似用户来进行计算的

采样：如果用户有较多的行为数据，说明已有一定兴趣模式，例如两个100维计算相似度，随机各取出10维度来计算，误差不大，更经济。Twitter的DIMSUMv2算法，并不是完全随机，和向量维度（总用户数）、相似度阈值、行向量的模（某个用户喜欢的数量多少）有关。如果单机可计算，使用OpenMP单机计算，发挥单机多核的能力，如果数据量较大（TB级别以上），使用Spark（中的RowMatrix）

#### 推荐计算

计算推荐分数，用MapReduce编程模型计算，在Map阶段：

* 遍历每个用户喜欢的物品
* 获取该用户的相似用户列表
* 吧每个喜欢的物品Map成两个记录发射出去。一个是键为<相似用户ID，物品ID，1>的三元组，可拼成一个字符串，值为<相似度>；另一个是键为<相似用户ID。物品ID，0>的三元组，其中1和0是区分两者，后面用到

Reduce阶段，求和后输出用<相似用户ID，物品ID，0>的值除以<相似用户ID，物品ID，1>的值

#### 调整

主要是调整用户对物品的喜欢程度

* 惩罚对热门数据的喜欢程度，因为热门很难反映用户真实兴趣，更可能是被煽动、无聊点击，群体性为
* 增加喜欢程度的时间衰减，一般使用一个指数函数，指数是一个负数，值和喜欢行为发生的时间间隔正相关
* 对用户相似度做平滑，行为记录较少的用户因为偶然喜欢上了相同物品，如果计算得到了较高相似度，是不值得信任的（？？？）

### 基于物品的协同过滤

场景：电商、喜欢了这部电影的人还喜欢...、关注了TA的人还关注了...。推荐你喜欢的物品相似的物品。简单直接有效

在基于内容的推荐系统中，相似物品是用内容的相似性计算的，但还会有一些内容特征抓不到的相似性。同一群体喜欢的物品总有某些共同的因素，虽然不能被表达出来，但能用来做推荐

首先计算相似物品，再根据用户消费的物品推荐相似的物品

同期的可推荐物品数量一般少于用户数量，所以计算物品相似度一般不会成为瓶颈；物品之间相似度不易改变，没有用户品味变化快，解耦了用户兴趣迁移的问题；物品对应的消费者数量大，所以计算出的物品相似度比用户相似度更可靠

同样基于用户物品关系矩阵

1. 构建用户物品关系矩阵，和user-based一样，通过用户行为（购买、点赞、收藏等）来构建
2. 两两计算相似度：例如行表示物品，列表示用户，则两两计算行向量之间的相似度，得到物品相似度矩阵，行和列都表示物品（和基于用户的协同过滤区别是一个计算行一个计算列向量，只需要将用户行为矩阵转置一下即可）
3. 根据场景不同推荐不同的结果：1）在某个物品页面推荐相关物品。2）猜你喜欢

#### 计算物品相似度

从用户物品关系矩阵得到的物品向量特点：

* 稀疏向量，没有消费过的物品就不再表达出来
* 一维表示一个用户，总维度代表总的用户数量
* 各个维度的值表示消费结果，可以是代表行为的0、1布尔值，也可以是量化的分数，如评分、时长、次数、费用

计算相似度使用余弦相似度，向量中大多数值是0，不需要计算，只计算两个物品的公共用户

#### 计算推荐结果