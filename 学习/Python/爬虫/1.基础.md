---
title: 1.基础
---

[TOC]

# HTTP
## Request HTTP Header
* Accept: text/plain
* Accept-Charset: utf-8（Python中默认是utf-8，如果不指定，网页可能默认返回GBK，在Python会是乱码）
* Accept-Encoding: gzip, deflate（对网页文本进行压缩，指定压缩方式，如果指定之后，接收到的是压缩包，还需要解压）
* Connection: keep-alive （进行通讯后，socket不关掉，还会继续请求数据，避免多次握手，在已经建立好的通道上继续请求）
* User-Agent: ...（指定什么客户端）
* Cookie: ...（需要登录的网站会使用到cookie）

## Response HTTP Header
* Location: http://.... （重定向，爬虫可能需要手动跳转）
* Set-Cookie: ... （服务端告诉客户端cookie）
* Status: 200 OK
    * 2XX 成功
    * 3XX 跳转
        * 302 重定向（urllib2已经对重定向做了处理，会自动跳转）
    * 4XX 客户端错误
        * 400 自己的错误，检查请求的参数或者路径
        * 401 未登录，需要授权的网页，尝试重新登录。
        * 403 如果是需要授权的网页，尝试重新登录。也可能IP被封，暂停爬取，并增加爬虫的等待时间，如果拨号网络，尝试重新联网更改IP
    * 500 服务器错误，直接丢弃，并计数，如果连接不成功，WARNING并停止爬取
        * 500 服务发生不可预期的错误
        * 503 服务器不能处理客户端的请求

# 爬取策略
## 宽度优先
第一层->爬取第一层的所有链接->爬取第二层的所有链接...
## 深度优先
第一层->第一层的第一个链接->第一层层第一个链接中的第一个链接...->第一层的第二的链接
## 策略
* 重要的网页距离种子站点比较近
* 万维网的深度并没有很深，一个网页有很多路径可以到达
* **宽度优先有利于多爬虫并行合作抓取**（因为使用队列）
* 深度与宽度相结合

# 抓取历史
## 如何记录抓取历史
> 要根据实际情况，都不是万能完美的方案

1. 将访问过的URL保存到数据库。*效率太低*
2. 用HashSet将访问过的URL保存起来。那只需接近O(1)的代价就可以查到一个URL是否被访问过了。*消耗内存*

3. URL经过MD5或SHA-1等单项哈希后再保存到HashSet或数据库。
    > MD5签名是一个哈希函数，可以将任意长度的数据转换为一个固定长度的数字（一般4个整型，16字节）、现实世界的URL组合必然超越哈希表的槽位数，因此碰撞一定存在，一般的HASH函数，例如Java的HashTable是一个Hash表再跟上一个链表，链表里存的是碰撞结果

4. Bit-Map方法， 建立一个BitSet，将每个URL经过一个哈希函数映射到某一位
    > 将URL的MD5值再次哈希，用一个或多个BIT位来记录一个URL
    > 在数据量在过亿的情况下可以使用Bloom Filter压缩，使用了多个哈希函数，高效、节省内存、碰撞率低

## 提高效率
* 评估网站的网页数量
    > 百度搜索site:www.mafengwo.cn就可以知道百度收录了马蜂窝多少网页，对于中国网站，百度比Google收录的多

* 选择合适的Hash算法和空间阈值，降低碰撞几率
* 选择合适的存储结构和算法

## 有效记录抓取历史
* 多数情况下不需要压缩，直接使用数据库，尤其网页数量少的情况（千万以下）
* 网页数量大的情况下，使用Bloom Filter压缩
* 重点是计算碰撞概率，并根据碰撞概率来确定存储空间的阈值
* 分布式系统（为了爬的快），将散列映射到多台主机的内存

# 网站结构分析
Robots.txt：对于爬虫的规范性协议，规定了哪些可以爬，哪些不要爬，可以帮助搜索引擎做优化，用搜索引擎来倒流。Disallow为禁止爬；其中的Sitmap为网站的结构，可以爬取Sitmap中的所有url

一般一个网站网址加上robots.txt就可以访问到

# XPath
> 查询DOM树

* 节点名 选取此节点的所有子节点，tag或*（选择任意的tag）
* / 从根节点选取，选择直接子节点，不包括更小的后代
* // 包含所有后代
* . 选取当前节点
* .. 选取当前节点的父节点
* @ 选取属性

    > 在DOM树，以路径的方式查询节点，通过@符号来选取属性
    `<a class="class1 class2"/>`
    可通过"//\*[@class='class1 class2']"来选取
    =符号要求属性完全匹配，可以用contains来部分匹配
    "//\*[contains(@class, 'class1')]"

## 运算符
* and or

    >选择P或者span或者h1标签的元素
    html.xpath('//td[@class="class1"]//\*[self::p or self::span or self::h1]')
    > 选择class为class1或者class2的元素
    html.xpath('//td[@class="class1" or @class="class2"]')

# 正则
| 符号 | 含义 |
| --- | --- |
| \ | 转义 |
| ^ | 字符串开始 ①smartgirl ②asmartgirl `^sm` ①成功②失败 |
| $ | 字符串结束 |
| * | 匹配前面子表达式0次或多次 |
| + | 匹配前面子表达式1次或多次 |
| ? | 匹配前面子表达式0次或1次，非贪婪模式，最多一次，不带问号为贪婪模式，会匹配整个 |
| {n,m} | 匹配至少n次，最多m次。{n}为固定n次|
| . | 匹配除\n之外的任何一个字符 |
| (pattern) | 匹配并获取这个匹配 smart `sm(.*)t`得到ar |
| [xyz] | 字符集合，匹配任意集合里的字符 | 
| [^xyz] | 排除集合里的字符，不能匹配 |
| \d | 匹配一个数字，等价[0-9] | 

## 举例
```
1. 提取标签中的正文
<span>正文</span>
'<span[^>]*>(.*?)</span>'
2. 查找特定类别的连接，例如/wiki/不包含Category目录
'<href="/wiki/(?!Category:)[^/>]*>(.*?)<'
3. 商品链接，https://item.jd.com/1234567.html
'/\d{7}.html'
```

# href \  \\\
* href="http://www.baidu.com"
    
    > http://是完整的URL，直接跳转
    
* href="\\\detail.taobao.com\111"

    > //是协议相关的绝对路径，如果现在是http://，则会变成http://detail.taobao.com。前面也可以是ftp、file，比较灵活
    
* href="\i\43214321.html"

    > /是网站的相对路径



                      