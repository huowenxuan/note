[TOC]

# HTTP
## Request HTTP Header
* Accept: text/plain
* Accept-Charset: utf-8（Python中默认是utf-8，如果不指定，网页可能默认返回GBK，在Python会是乱码）
* Accept-Encoding: gzip, deflate（对网页文本进行压缩，指定压缩方式，如果指定之后，接收到的是压缩包，还需要解压）
* Connection: keep-alive （进行通讯后，socket不关掉，还会继续请求数据，避免多次握手，在已经建立好的通道上继续请求）
* User-Agent: ...（指定什么客户端）
* Cookie: ...（需要登录的网站会使用到cookie）

## Response HTTP Header
* Location: http://.... （重定向，爬虫可能需要手动跳转）
* Set-Cookie: ... （服务端告诉客户端cookie）
* Status: 200 OK
    * 2XX 成功
    * 3XX 跳转
        * 302 重定向（urllib2已经对重定向做了处理，会自动跳转）
    * 4XX 客户端错误
        * 400 自己的错误，检查请求的参数或者路径
        * 401 未登录，需要授权的网页，尝试重新登录。
        * 403 如果是需要授权的网页，尝试重新登录。也可能IP被封，暂停爬取，并增加爬虫的等待时间，如果拨号网络，尝试重新联网更改IP
    * 500 服务器错误，直接丢弃，并计数，如果连接不成功，WARNING并停止爬取
        * 500 服务发生不可预期的错误
        * 503 服务器不能处理客户端的请求

# 爬取策略
## 宽度优先
第一层->爬取第一层的所有链接->爬取第二层的所有链接...
## 深度优先
第一层->第一层的第一个链接->第一层层第一个链接中的第一个链接...->第一层的第二的链接
## 策略
* 重要的网页距离种子站点比较近
* 万维网的深度并没有很深，一个网页有很多路径可以到达
* **宽度优先有利于多爬虫并行合作抓取**（因为使用队列）
* 深度与宽度相结合

# 抓取历史
## 如何记录抓取历史
> 要根据实际情况，都不是万能完美的方案

1. 将访问过的URL保存到数据库。*效率太低*
2. 用HashSet将访问过的URL保存起来。那只需接近O(1)的代价就可以查到一个URL是否被访问过了。*消耗内存*

3. URL经过MD5或SHA-1等单项哈希后再保存到HashSet或数据库。
    > MD5签名是一个哈希函数，可以将任意长度的数据转换为一个固定长度的数字（一般4个整型，16字节）、现实世界的URL组合必然超越哈希表的槽位数，因此碰撞一定存在，一般的HASH函数，例如Java的HashTable是一个Hash表再跟上一个链表，链表里存的是碰撞结果

4. Bit-Map方法， 建立一个BitSet，将每个URL经过一个哈希函数映射到某一位
    > 将URL的MD5值再次哈希，用一个或多个BIT位来记录一个URL
    > 在数据量在过亿的情况下可以使用Bloom Filter压缩，使用了多个哈希函数，高效、节省内存、碰撞率低

## 提高效率
* 评估网站的网页数量
    > 百度搜索site:www.mafengwo.cn就可以知道百度收录了马蜂窝多少网页，对于中国网站，百度比Google收录的多

* 选择合适的Hash算法和空间阈值，降低碰撞几率
* 选择合适的存储结构和算法

## 有效记录抓取历史
* 多数情况下不需要压缩，直接使用数据库，尤其网页数量少的情况（千万以下）
* 网页数量大的情况下，使用Bloom Filter压缩
* 重点是计算碰撞概率，并根据碰撞概率来确定存储空间的阈值
* 分布式系统（为了爬的快），将散列映射到多台主机的内存

# 网站结构分析
Robots.txt：对于爬虫的规范性协议，规定了哪些可以爬，哪些不要爬，可以帮助搜索引擎做优化，用搜索引擎来倒流。Disallow为禁止爬；Sitmap为网站的结构，可以爬取Sitmap中的所有url

