# 大型网站 技术架构演进与性能优化 （2018）
[TOC]

## 前言
随着无线技术的发展，最具代表性的新技术就是Node，但是Node并没有达到想象中的发展前景

## 1 构建大型网站：分布式改造
1. 微服务化：将大量粗粒度的应用逻辑拆小做服务化改造
2. 建立分布式服务框架：分布式配置系统、分布式RPC框架、异步笑嘻嘻系统、分布式数据层、分布式文件系统、服务的发现、注册和管理
3. 解决状态一致性问题

### 典型的分布式架构
分布式架构能解决两个方向的扩展
1. 横向扩展：解决架构上的容量问题
2. 纵向扩展：解决业务的扩展问题

- 服务化改造：从业务架构的角度出发，将业务做耕细粒度的功能拆分，使业务逻辑更加清晰、边界更加清楚且易于维护；通过接口标准化提供统一的访问方式
- 分布式化改造：更多从系统架构层面的角度出发，更多看请求的访问路径，即一个请求必须先访问什么再访问什么、一次访问要经过哪些步骤才能有最终结果等

### 分布式配置框架
两种管理方式
1. 拉取模式
2. 推送模式

### 分布式RPC框架
有了RPC，进行分布式改造将会事半功倍。RPC框架需要实现：1）服务的注册。2）服务发现。3）服务调度和负载均衡。4）统一的SDK封装。Java的Dubbo、HSF都比较成熟

### 分布式消息框架
1. 实时消息：RocketMQ、Apache Kafka
    1. 异步解耦：分开调用者和被调用者的处理逻辑、降低系统耦合、解决处理语言之间、数据结构之间的差异、生产者消费者的速度差异。要保证最终的一致性、消息的有序性
    2. 多消费端：一个消息，多个订阅端
2. 延时消息：如订单15分钟未付款自动关闭，使用延时消息队列。

### 分布式数据层
### 分布式文件系统
### 应用的服务化改造
1. 应用分层设计：最起码把数据库的访问统一抽象出来形成数据层，而不是直接在代码里写sql——会使重构和水平拆分数据库非常麻烦。通常从垂直方向划分应用，分成服务层、业务逻辑层、数据层。如果业务要修改，必须清楚的知道到哪个层去完成（分层的职责是否清晰）；某个层进行修改，尽量避免其他层也需要修改
2. 微服务化：把服务分得更细，每个业务只负责一个功能单元，形成单一职责提升系统可维护性、扩展性和开发效率

## 2 无线化：无线时代下的架构演进
### 无线链路的优化
PC更重要的是优化首屏加载，无线端更多则是优化中间的管道

1. 无线端请求合并收益较大，将当前的两次请求在服务端做ESI合并为一个请求，会明显减少总耗时
2. 无线端数据大小对性能的影响比PC更明显

WebP图片优化：文件更小但是达到和JPEG格式相同的质量，Chrome支持，Android4.0以上支持

Node带来的新思路（都不好）
1. Node作为单独的一层：将MVC的View层独立出来交给Node完成，JavaWeb只提供JSON接口供Node调用。带来的问题是Node层和JavaWeb服务器层有点重复
2. Node MVC框架完全替换JavaWeb的MVC
3. 在Java中运行Node：解析Node

### Node和Java的比较
1. 语言特性：JS语法简单，容易编写基于事件驱动的实现，但是面向对象的描述能力偏弱，对数据类型的定义也比较单一。Java更擅长构建复杂逻辑的大型应用。在运行效率上，JS是解释型，Java是编译执行，但Node做了优化，所以差别不大
2. 开发效率
    * 语言复杂度：都不需要关心内存管理，都是基于虚拟机来管理内存。并发角度：JS基于事件，Java基于线程，所以JS占优势。JS是无阻塞I/O，在I/O效率上比Java有优势
    * 程序员培养：Java程序员招聘比JS更容易
    * 开发工具包：Java工具类库非常丰富，想要的任何工具都可以找到。JS的工具主要在前端，虽然Node社区也很活跃，但是要达到Java的规模尚需时日
    * 编码效率：Java部署效率差，JS测试更简单，但是debug机制不完善

## 3 大型网站平台化演进：大中台小前台（但是这一章只有中台）
- 为什么需要中台：中台的目标是为了解决效率问题，同时降低创新成本。随着生态复杂度、业务复杂度、系统复杂度的提升，每个业务的执行都是跨领域的，设计会员、商品、交易、店铺、营销、评价、支付、物流、售后等，一套业务逻辑横跨几十个系统，时间一长就没人能说清了全局了。此时，中台化可解决信息获取成本高、互联互通成本高、服务具有不确定性和低水平重复建设这四个问题

- 什么是中台：通过**制定标准和机制**，把不确定的业务规则和流程通过工业化和市场化的手段确定下来，减少沟通成本，提升协作效率。中台的目标是要向上层业务提供这些基础的服务，必须能说清楚自己有那些服务、数据和功能，把他们统称为能力。所以需要能够定义能力（标准和规范）、能力的发现、注册、列表、评价和更新机制。需要能够对接能力，自己并不负责具体的业务（到最后也不知道到底是干什么的）

## 4 全球化下的网站演进：全球部署方案
全球化的关键问题：
1. 业务核心单元的梳理：和细心单元必须需可裁剪可添加
2. 核心单元可快速部署到国外的机房，最好能一键部署，即首先要实现单元化部署
3. 实现全球数据连通
4. 良好的扩展性和定制型

业务上的挑战
1. 就近访问
2. 不能跨区域进行大流量的并发读写

国际化部署的目标
1. 单元化：1）业务单元内闭环：单元内所有业务自封闭，包含业务所需的所有服务和所依赖的全部数据。2）单元化部署
2. 如果业务都由总部维护，要实现一套代码、全球部署
3. 服务本地、数据共享：服务本地为就近读写，不跨国。数据共享为一次发布，卖到全球
4. 区域容灾、全球多活

### Sequence ID的冲突问题
跨区域Sequence解决思路
1. 一个全局统一的Sequence生成器，生成器来分配每个库的Sequence分段。最大缺点就是“统一”，给维护带来问题
2. 提前预设分段，比如A库用奇数，B库用偶数；基于每个数字做Hash；美国用0~10亿；中国用10~20亿。缺点是比较死，更改不方便而且有上限
3. 设置起始值加步长，并且基于数据库表做更新。比如中美欧三个数据库起始值分别是0、1000、2000，步长都是1000，每次生成Sequence就加1000，如果B库用尽后，三个库的起始值分别变为0、4000、2000。这种方法灵活性好，增加数据库也不需要调整，不需要统一的控制中心，不存在维护的问题，比较理想

### 多语言问题
1. 系统文案多语言：比较简单（i18n？）
2. 用户提交数据的翻译问题（如商品描述），要用到翻译引擎
    - 存储有两种思路：
        1. 多语言版本只是存在cache中
        2. 模型层解决：数据库储存多语言。将具有多语言的字段独立出来扩展存储，再通过id做标识，在不同的区域选择不同的商品id，这个转换工作可能是在商品发布阶段就实现
    - 翻译引擎
        1. 浏览器加载页面时直接调用在线翻译工具如Google翻译
        2. 搜索服务中，将用户输入的语言翻译成后端可识别的语言
3. 在数据模型层解决多货币、多价格、尺码等需求，还会涉及业务逻辑的差异问题

### 多时区问题
> UTC时间：世界标准时间，与时区无关，以格林尼治时间1970开始计算
> 时区：24个时区，两个时区之间相差一小时

解决思路：
1. 服务器时间、数据存储时间、服务层在全球部署时统一时区，不做时区相关的转换
2. 时区相关的转换在表现层和控制层做，都依赖于统一的时区转换工具
3. 系统需要感知访问者的时区，可通过用户ip或者用户自己选择的时区来实现

方案一，使用UTC时间
1. 数据库使用long存储UTC时间，程序中使用long型UTC时间进行计算
2. 服务处、存储层直接使用long型计算、展示层根据业务对时间做本地化转换；控制层对时间参数进行处理，将其转换成UTC时间
3. 为控制层提供本地时间计算服务，对下游系统，直接传递long型，对上游系统，将标准时间转换成long处理
优点在于服务层、存储层不需要转换时区，只有展示层需要。缺点是存储时需要转为long，展示层需要将long转成用户本地时间

方案二，使用本地时间
1. 各服务器、数据库、应用使用统一的本地时间
2. 服务层、存储层保持date型，展示层不对时间做本地化处理，展示服务器的本地时间
3. 尽量不做全球化数据同步，同步时尽量使用long型
优点是不需要都不需要转换时区。缺点在于跨时区需要自行计算本地时间

方案三，使用同一时区
1. 全平台使用同一时区的时间
2. 服务层、存储层不对时间进行转换，展示层对时间进行本地化换算；控制层处理时间参数，将他转换成同一时区时间；为控制层提供本地时间计算服务
有点在于服务层、存储层不需要修改，数据同步不需要关心时区问题。缺点是要修改表示层，本地时间计算服务会增加；需要感知用户所在地区

### 通用版与定制版的选择
各国的差异性
1. 法律法规：有些国家不能售卖某些商品；有些国家的私人数据不能外流
2. 业务规则存在差异：某些商品的类目；度量单位的差异导致鞋子、衣服的尺码不同
3. 部署和维护的成本

要使用通用版还是定制版，搞清楚目标是否都是同一个人维护一套代码
- 如果是，就选择通用版，可以逐步实现差异化
- 如果不是，是否是同一套代码就不重要了，更重要的是数据的打通

## 5 应用程序优化：代码级优化（应用服务器端的优化）
优化思路：CPU、内存、网络、磁盘等总会有一个先达到瓶颈，只有先优化最先达到瓶颈的才会产生实际效果。
如何发现瓶颈：看哪里消耗了最多的CPU时间、从这里做优化效果更明显。使用压测工具（Apache的ab压测工具等测读系统；全链路压测技术测写系统）、发现代码热点的工具

## 6 应用架构探索：合并部署
将多个应用程序许下（.war包）同时部署在一个Tomcat实例中。共享同一个JVM进程，每个应用之间通过ClassLoader隔离，但是应用之间RPC调用不通过网络，而是走Java的本地调用。能提升网络性能、减少Java应用调用之前的序列化和反序列化（没用）

## 7 链路优化：大秒系统的极致优化思路
特点：
1. 整个页面Cache在用户浏览器
2. 如果强制刷新，也会请求到CDN
3. 实际有效请求只是“购买”按钮
### 热点隔离
秒杀系统的第一个设计原则就是隔离热点数据，禁止1%的请求影响剩余99%的请求，隔离之后议会对1%的请求针对性优化。
1. 业务隔离
2. 系统隔离：分组部署把1%的请求隔离开。可申请单独的域名，将不同的请求落入不同的集群
3. 数据隔离：启动单独的cache集群或mysql数据库来释放热点数据

实现隔离：
1. 调用不同的服务接口
2. 在数据层给数据打上特殊的标签来区分

### 动静分离
静态化（所有的不包含都是HTML源码中不包含）
1. 一个页面对应一个URL
2. 页面中不包含浏览者的相关因素。如HTML代码中不能含有用户姓名、身份表示、Cookie等，这里不包括js动态生成的部分
3. 页面中不包含时间相关因素（指服务器输出的时间，而不是浏览器获取的时间），不能随着时间的变化导致dom变化，例如到某个时间点，页面中的“立即购买”就可以使用
4. 不包含地域因素：不同地区运费不同，如果要做成静态化，运费就不能直接反映在html中
5. 不能包含cookie等私有数据

解决的问题：改变了缓存位置，缓存在Web服务器（Nginx、Apache）层，而不是Java层，屏蔽了Java的一些弱点，使请求尽量不经过Java，而在Web服务器就直接返回。直接缓存HTTP连接而不仅仅是缓存数据，能最快地获取数据

如何改造为静态
1. URL唯一化。详情系统天然可做到URL唯一化，例如item.html?id=xxx
2. 分离浏览者相关的因素，去掉登录信息，可通过动态请求来获取
3. 分离时间因素，通过动态请求来获取
4. 异步化地域因素，通过动态请求来获取
5. 去掉Cookie

### 静态化方案的选择
想清楚以下问题：
1. 是否一致性Hash：**缓存和命中率紧密相关，命中率又和数据集中度相关，数据集中就必须要求一致性Hash**，但是一致性Hash天然缺陷就是会产生热点，可能会造成网络瓶颈
2. 是否ESI：对性能有影响但是对客户端友好，对前端编程也方便
3. 是否使用物理机：可提供更大内存、更好CPU。但是会导致集群相对集中，增加网络风险，对Java而言，内存增加并没有巨大好处
4. 谁来压缩
5. 网卡选择：成本问题，有钱就选择万兆网卡和交换机

三种方案：
1. 实体机单机部署：较理想，增大了Cache内存，采用一致Hash分组来提升命中率；减轻Cache失效压力
2. 统一Cache层：更理想，减小运维成本，方便其他静态化系统接入；易于cache统一维护；可共享内存、最大化利用内存；更有助于安全防护
3. 上CDN（不好做）

### 如何失效
被动和主动相结合
1. 被动失效：使用cache时长来自动失效、手工设置失效
2. 主动失效
    1. cache失效中心监控数据库表变化，发送purge失效请求（承担主要作用）
    2. 根据时间戳失效装修内容
    3. Java系统发布清空cache
    4. VM模板发布清空cache

### 数据分层校验
在不同层次、不断尽可能过滤掉无效请求
1. 数据的动静分离
2. 将90%的数据缓存在客户端浏览器
3. 将动态请求数据cache在web端
4. 对读数据不做强一致性校验
5. 对写数据进行基于时间的合理分片（验证码等，一般控制在n秒以上才会下单，防秒杀器）
6. 对写请求做限流保护（通过锁或队列控制瞬间的请求量）
7. 对写数据进行强一致性校验

### 优化
1. Java处理动态请求优化：将动态请求缓存到Nginx或Web代理服务器
2. 同一商品被大量并发的问题，除了将热点信息放在Tair缓存里面，还需要解决热点商品这类单点瓶颈，采用应用层的Localcache
    1. 像商品标题、描述等不变的数据在秒杀开始之前全量推送到秒杀机器上，缓存到秒杀结束
    2. 像库存这类会修改的数据是缓存一定时间（几秒），失效后再去Tair中拉取。真正下单时判断数据一致性
3. 统一数据大并发更新问题（库存更新）：热点隔离（将热点商品放在单独的热点库，还需要做动态迁移）不能解决并发锁的问题，有两种办法：
    1. 应用层做排队
    2. 在数据库层做排队。应用层只能做到单机的排队，如果应用层机器数量多，效果还是有限，在数据库层排队是最理想的。MySQL的InnoDB层上的patch可做到在数据库层上对单行记录并发排队
    
## 8 全局基础设施优化：资源调度优化
## 9 网站高可用建设：大型网站的稳定性建设
- 压测——单系统压测：1）引流：将线上集群中的流量几种到少部分的机器上，得出单机的极限性能，从而计算出整个集群的性能，不会产生额外的测试数据，对读写系统都合适。2）放大流量：使用TCPCopy工具把一个请求copy出多个重复的请求
- 压测——全链路压测：目前比较好的、可制造出线上大流量的手段，优点在于能串联线上全部系统，并让每个系统同时达到流量峰值，适用的场景更多，实施成本较高
- 监控——调用链路跟踪新系统：记录一次调用所涉及到的所有系统和数据库。每个请求带一个trace信息，把trace一直鞋带到请求的所有调用链路上，打印到日志中，并通过日志中的trace关联信息形成一个调用栈
- 监控——业务数据轨迹重现系统：追踪数据的所有变更记录，了解是谁在哪台机器上调用了哪个接口、哪个时间点从某个数据变更成另一个数据的完整记录。主要记录写操作的操作日志，依赖中间件，在中间件里添加一些钩子，把钩子封装在Client中，通过配置系统推送规则，控制哪些请求需要记录日志，给请求添加标记，一直追踪到数据库上。还需要一个中间件，解析数据库的binglog、感知数据库的update、delete操作，并记录上变更之前和之后的数据，最终这些变更记录都写到HBase里，便于检索和查询。可实现商品的快照功能
